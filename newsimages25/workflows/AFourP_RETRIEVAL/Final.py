# -*- coding: utf-8 -*-
"""AFourP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G_bdOLi3k36i9COs5rEF9Fi5WWctdzD2
"""

!pip install --upgrade --quiet transformers accelerate timm open_clip_torch ftfy regex tqdm

import os
import torch
import pandas as pd
from PIL import Image
from tqdm import tqdm
import open_clip
import transformers

print(f"Transformers version: {transformers.__version__} (must be >=4.26)")

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/MediaEval - AFourP/newsarticles.csv")

IMAGE_FOLDER = "/content/drive/MyDrive/MediaEval - AFourP/newsimages"

print("Loading ViT-bigG-14 OpenCLIP model...")
clip_model, _, preprocess = open_clip.create_model_and_transforms(
    'ViT-bigG-14', pretrained='laion2b_s39b_b160k'
)
clip_model = clip_model.to(device)
tokenizer = open_clip.get_tokenizer('ViT-bigG-14')

image_embeddings = {}
print("ðŸ”„ Computing OpenCLIP image embeddings...")
for file in tqdm(os.listdir(IMAGE_FOLDER)):
    if not file.endswith(".jpg"):
        continue
    image_id = file.split(".")[0]
    img_path = os.path.join(IMAGE_FOLDER, file)
    try:
        image = preprocess(Image.open(img_path).convert("RGB")).unsqueeze(0).to(device)
        with torch.no_grad():
            emb = clip_model.encode_image(image)
            emb /= emb.norm(dim=-1, keepdim=True) # Normalize the embedding
        image_embeddings[image_id] = emb.cpu()
    except:
        pass # Ignore corrupted or unreadable images

print(f"Stored embeddings for {len(image_embeddings)} images.")

!pip install --upgrade --quiet transformers accelerate timm

use_blip_itm = False
try:
    from transformers import BlipProcessor, BlipForImageTextMatching
    blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-itm-base-coco")
    blip_model = BlipForImageTextMatching.from_pretrained("Salesforce/blip-itm-base-coco").to(device)
    use_blip_itm = True
    print("Using BLIP ITM reranker (Salesforce/blip-itm-base-coco)")
except ImportError:
    print("BLIP ITM not available. Falling back to CLIP similarity reranker.")

# Function to create a text prompt from article data
def build_prompt(row):
    title = str(row["article_title"])
    tags = ",".join(str(row["article_tags"]).split(";")[:5])  # keep top 5 tags
    return f"News headline: {title}. Related topics: {tags}."

# Fallback function to rerank images using CLIP if BLIP is unavailable
def rerank_with_clip(text, top_images):
    text_tokens = tokenizer([text])
    with torch.no_grad():
        text_emb = clip_model.encode_text(text_tokens.to(device))
        text_emb /= text_emb.norm(dim=-1, keepdim=True)

    best_img, best_score = None, -1
    for img_id, img_emb, _ in top_images:
        sim = torch.cosine_similarity(text_emb.cpu(), img_emb).item()
        if sim > best_score:
            best_score = sim
            best_img = img_id
    return best_img, best_score

results = []
similarities = []

# Setup for output files
group_name = "ShivShiv2"
approach_name = "OPENCLIP_BLIPITM" if use_blip_itm else "OPENCLIP_CLIPRERANK"
out_folder = f"{group_name}/RET_{approach_name}_LARGE"
os.makedirs(out_folder, exist_ok=True)

print(f"Running retrieval with {approach_name} reranker...")

for _, row in tqdm(df.iterrows(), total=len(df)):
    text = build_prompt(row)

    # Stage 1: Generate text embedding and find top-5 similar images using OpenCLIP
    text_tokens = tokenizer([text])
    with torch.no_grad():
        text_emb = clip_model.encode_text(text_tokens.to(device))
        text_emb /= text_emb.norm(dim=-1, keepdim=True)

    scored_embeddings = [
        (img_id, img_emb, torch.cosine_similarity(text_emb.cpu(), img_emb).item())
        for img_id, img_emb in image_embeddings.items()
    ]
    top_5 = sorted(scored_embeddings, key=lambda x: x[2], reverse=True)[:5]

    # Stage 2: Rerank the top-5 images with BLIP ITM model for a more accurate score
    if use_blip_itm:
        best_img, best_score = None, -1
        for img_id, _, _ in top_5:
            img_path = os.path.join(IMAGE_FOLDER, img_id + ".jpg")
            image = Image.open(img_path).convert("RGB")
            inputs = blip_processor(images=image, text=text, return_tensors="pt").to(device)
            with torch.no_grad():
                outputs = blip_model(**inputs)
                probs = outputs.logits.softmax(dim=1)
                score = probs[:, 1].item()
            if score > best_score:
                best_score = score
                best_img = img_id
        best_img_id, rerank_score = best_img, best_score
    else:
        # Use the CLIP reranker as a fallback
        best_img_id, rerank_score = rerank_with_clip(text, top_5)

    similarities.append(rerank_score)

    # Save the best-matching image
    img = Image.open(os.path.join(IMAGE_FOLDER, best_img_id + ".jpg")).convert("RGB")
    img = img.resize((460, 260))
    img.save(f"{out_folder}/{row['article_id']}_{group_name}_{approach_name}.png")
    results.append((row['article_id'], best_img_id, rerank_score))

# Calculate and print the average matching score
avg_score = sum(similarities) / len(similarities)
print(f"\nAverage match score ({approach_name}): {avg_score:.4f}")

# Save results to a CSV file
results_df = pd.DataFrame(results, columns=["article_id", "retrieved_image_id", "score"])
results_df.to_csv(f"{group_name}_retrieval_results.csv", index=False)
print(f"Results saved to {group_name}_retrieval_results.csv")
print(f"Submission images saved in {out_folder}")

# Display a visual preview of the first 3 results
from IPython.display import display
print("\n Previewing first 3 retrievals:")
for _, row in results_df.head(3).iterrows():
    article_row = df[df["article_id"] == row["article_id"]].iloc[0]
    prompt = build_prompt(article_row)
    print(f"\n {article_row['article_title']}")
    print(f"Prompt: {prompt}")
    print(f"Retrieved: {row['retrieved_image_id']} | Score: {row['score']:.4f}")
    img_path = os.path.join(IMAGE_FOLDER, row['retrieved_image_id'] + ".jpg")
    display(Image.open(img_path).resize((460, 260)))

import os

print(f"\nRenaming images in '{out_folder}'...")
for filename in os.listdir(out_folder):
    if filename.endswith(".png") and "ShivShiv2" in filename:
        new_filename = filename.replace("ShivShiv2", "AFourP")
        old_filepath = os.path.join(out_folder, filename)
        new_filepath = os.path.join(out_folder, new_filename)
        os.rename(old_filepath, new_filepath)
        # print(f"Renamed '{filename}' to '{new_filename}'") # Uncomment to see each rename

print("Image renaming complete.")

import shutil

# Define the destination folder in Google Drive
drive_output_folder = "/content/drive/MyDrive/MediaEval - AFourP/FinalResults"
os.makedirs(drive_output_folder, exist_ok=True)

# Copy the output folder to Google Drive
shutil.copytree(out_folder, os.path.join(drive_output_folder, os.path.basename(out_folder)))

# Copy the results CSV to Google Drive
shutil.copy(f"AFourP_retrieval_results.csv", drive_output_folder)

print(f"\nOutput folder '{out_folder}' and results CSV 'AFourP_retrieval_results.csv' copied to '{drive_output_folder}' in Google Drive.")

!ls -R "/content/drive/MyDrive/MediaEval - AFourP/FinalResults"

!find "/content/drive/MyDrive/MediaEval - AFourP/Results2" -mindepth 2 -type f \
  | awk -F/ '{print $(NF-1)}' | sort | uniq -c

import shutil

# Zip the Results2 folder
shutil.make_archive("/content/FinalResults", 'zip', "/content/drive/MyDrive/MediaEval - AFourP/FinalResults")

# Download to PC
from google.colab import files
files.download("/content/FinalResults.zip")