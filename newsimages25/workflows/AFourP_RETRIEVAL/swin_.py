# -*- coding: utf-8 -*-
"""Swin .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ew_Ro_Do4YXeYuUjdNTG2-vc34lfj6t4
"""

!pip install --upgrade --quiet transformers accelerate timm open_clip_torch ftfy regex tqdm

import os
import torch
import pandas as pd
from PIL import Image
from tqdm import tqdm
import open_clip
import transformers

print(f"Transformers version: {transformers.__version__}")

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/MediaEval - AFourP/newsarticles.csv")

IMAGE_FOLDER = "/content/drive/MyDrive/MediaEval - AFourP/newsimages"

print("Loading Swin Transformer model...")
from transformers import SwinModel, AutoImageProcessor

# Choose Swin backbone (you can try base, tiny, or large)
swin_model_name = "microsoft/swin-base-patch4-window7-224"
swin_processor = AutoImageProcessor.from_pretrained(swin_model_name)
swin_model = SwinModel.from_pretrained(swin_model_name).to(device)

image_embeddings = {}

print("Computing Swin image embeddings...")
for file in tqdm(os.listdir(IMAGE_FOLDER)):
    if not file.endswith(".jpg"):
        continue
    image_id = file.split(".")[0]
    img_path = os.path.join(IMAGE_FOLDER, file)

    try:
        image = Image.open(img_path).convert("RGB")
        inputs = swin_processor(images=image, return_tensors="pt").to(device)

        with torch.no_grad():
            outputs = swin_model(**inputs)
            emb = outputs.pooler_output        # [batch_size, hidden_dim]
            emb = emb / emb.norm(dim=-1, keepdim=True)  # normalize

        image_embeddings[image_id] = emb.cpu()
    except Exception as e:
        print(f"Error processing {image_id}: {e}")

print(f"Stored embeddings for {len(image_embeddings)} images.")

import open_clip

print("Loading OpenCLIP text model (ViT-L-14)...")
# Using the Large model instead of Giant
clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='laion2b_s32b_b82k')
clip_model = clip_model.to(device)
tokenizer = open_clip.get_tokenizer('ViT-L-14')

print("Model loaded successfully.")

import open_clip
import numpy as np
from IPython.display import display
from tqdm.notebook import tqdm

# --- 2. Re-compute Image Embeddings using the OpenCLIP image encoder (Corrected) ---
image_embeddings = {}
print("\nComputing OpenCLIP image embeddings...")
error_count = 0
for file in tqdm(os.listdir(IMAGE_FOLDER)):
    if not file.endswith(".jpg"):
        continue
    image_id = file.split(".")[0]
    img_path = os.path.join(IMAGE_FOLDER, file)
    try:
        image = preprocess(Image.open(img_path).convert("RGB")).unsqueeze(0).to(device)
        with torch.no_grad():
            emb = clip_model.encode_image(image)
            emb /= emb.norm(dim=-1, keepdim=True)
        image_embeddings[image_id] = emb.cpu()
    except Exception as e:
        # This will now print the error instead of silently passing
        print(f"Warning: Could not process {image_id}. Error: {e}")
        error_count += 1

print(f"\nStored embeddings for {len(image_embeddings)} images.")
if error_count > 0:
    print(f"Skipped {error_count} images due to errors.")

# --- Add a check to ensure embeddings were created ---
if not image_embeddings:
    raise ValueError("The 'image_embeddings' dictionary is empty. Please check the IMAGE_FOLDER path and ensure the images are accessible and not corrupt.")

# --- 3. Helper Function to Create Prompts ---
def build_prompt(row):
    """Creates a text prompt from article title and tags."""
    title = str(row["article_title"])
    tags = ",".join(str(row["article_tags"]).split(";")[:5])
    return f"News headline: {title}. Related topics: {tags}."


# --- 4. Main Retrieval Loop (this will now work correctly) ---
results = []
similarities = []
print("\nRunning retrieval loop...")

for _, row in tqdm(df.iterrows(), total=len(df)):
    text_prompt = build_prompt(row)
    article_id = row['article_id']

    text_tokens = tokenizer([text_prompt]).to(device)
    with torch.no_grad():
        text_emb = clip_model.encode_text(text_tokens)
        text_emb /= text_emb.norm(dim=-1, keepdim=True)

    best_score = -1
    best_image_id = None
    text_emb_cpu = text_emb.cpu()

    for img_id, img_emb in image_embeddings.items():
        # This calculation will now work because both embeddings are the same size
        sim = (text_emb_cpu @ img_emb.T).item()
        if sim > best_score:
            best_score = sim
            best_image_id = img_id

    similarities.append(best_score)
    results.append((article_id, best_image_id, best_score))

print(f"\nRetrieval complete for {len(df)} articles.")

# --- Calculate and Print Average Score ---
avg_score = sum(similarities) / len(similarities) if similarities else 0
print(f"\nAverage match score: {avg_score:.4f}")

# --- Save Full Results to CSV ---
group_name = "ThirThir"
results_df = pd.DataFrame(results, columns=["article_id", "retrieved_image_id", "score"])
results_df.to_csv(f"{group_name}_retrieval_results.csv", index=False)
print(f"Results saved to {group_name}_retrieval_results.csv")

# --- Display a Visual Preview of the First 3 Results ---
print("\n--- Previewing first 3 retrievals ---")
for _, row in results_df.head(3).iterrows():
    # Find the original article data
    article_row = df[df["article_id"] == row["article_id"]].iloc[0]
    prompt = build_prompt(article_row)

    print(f"\nüì∞ Title: {article_row['article_title']}")
    print(f"üìù Prompt: {prompt}")
    print(f"üñºÔ∏è Retrieved: {row['retrieved_image_id']} | Score: {row['score']:.4f}")

    # Display the retrieved image
    img_path = os.path.join(IMAGE_FOLDER, row['retrieved_image_id'] + ".jpg")
    try:
        display(Image.open(img_path).resize((460, 260)))
    except FileNotFoundError:
        print(f"Image not found: {img_path}")