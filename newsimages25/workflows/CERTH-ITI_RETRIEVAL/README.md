# NewsImages-MediaEval2025 CERTH-ITI

Official implementation (Instructions, code and models) of the CERTH-ITI participation on the  MediaEval2025 NewsImages task.
## Introduction
This repository presents the CERTH-ITI approach to the NewsImages 2025 challenge, which focuses on recommending semantically appropriate images for news articles. Our method employs a two-stage retrieval–reranking architecture:

1. **Retrieval** – multiple vision–language models are used to retrieve candidate images.

2. **Reranking** – a large multimodal language model (MLLM) refines the ranking by incorporating article–image contextual alignment.

This design leverages the complementary strengths of diverse vision–language models for broad candidate coverage and a powerful MLLM for context-aware reranking, resulting in more accurate and relevant image recommendations.

### Stage 1: Image Retrieval
Multiple vision-language model families are used: CLIP, BLIP, BLIP2, SLIP, and BEiT3. For each family, several pre-trained models are used. For each model, both the article text and candidate images from the test dataset are encoded into a joint embedding space. Cosine similarities are computed, the scores are aggregated, and the top-N images are retrieved. To calculate textual embeddings, we utilize four different levels of the article corpus.
- The article  **title**.
- The **summary** of the article text generated by the LLaMA LLM.
- A **generated image caption** also produced by the LLaMA LLM.
- The provided **reference images**, where similarities are computed directly against the test dataset.
### Stage 2: Reranking
For each article, the retrieved images are refined using**Qwen2.5**, a large multimodal model capable of evaluating fine-grained semantic alignment. Qwen2.5 produces contextual relevance scores, and the list is reordered accordingly.

## Installation

```bash
conda create -n newsimages2 python=3.10 -y

conda activate newsimages2
conda install pytorch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 pytorch-cuda=12.4 -c pytorch -c nvidia
pip install salesforce-lavis
pip install transformers accelerate bitsandbytes sentencepiece
pip install git+https://github.com/openai/CLIP.git
pip install -r requirements.txt
pip install "protobuf<4"
```


## Pre-trained models download
Download the following pretrained weights and place them in the corresponding folders:

- BEIT3: Download the weights for COCO and Flickr30k Retrieval from the [official beit3 repository](https://github.com/microsoft/unilm/tree/master/beit3#fine-tuning-on-coco-and-flickr30k-retrieval-image-text-retrieval) → `models/beit3/checkpoints`.
- BLIP: Download the [official](https://github.com/salesforce/BLIP) weights for Image-Text Matching → `models/BLIP/pre_trained_models`.
  - [model_base_retrieval_coco](https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth)
  - [model_base_retrieval_flickr](https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_flickr.pth)
  - [model_large_retrieval_coco](https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_coco.pth)
  - [model_large_retrieval_flickr](https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_flickr.pth)
- SLIP: Download the pre-trained models from the [official SLIP repository](https://github.com/facebookresearch/SLIP?tab=readme-ov-file#results-and-pre-trained-models) → `models/SLIP/pre_trained_models`.
  - [slip_base_100ep](https://dl.fbaipublicfiles.com/slip/slip_base_100ep.pt)
  - [slip_large_100ep](https://dl.fbaipublicfiles.com/slip/slip_large_100ep.pt)
  - [slip_small_100ep](https://dl.fbaipublicfiles.com/slip/slip_small_100ep.pt)
  - [slip_base_cc3m_40ep](https://dl.fbaipublicfiles.com/slip/slip_base_cc3m_40ep.pt)
  - [slip_base_cc12m_35ep](https://dl.fbaipublicfiles.com/slip/slip_base_cc12m_35ep.pt)

## Execution Instructions
1. Articles summarization and image caption recommendation:
```bash
cd 00_preprocessing
python Summarization_captioning.py
```
2. Image feature extraction: 
  - Copy the image dataset into `/path/to/your/images/` folder and prepare the `dataset_paths.txt` text file with the absolute paths of your images containing **one image path per line**.  
For example:

```txt
/path/to/your/images/image_1.jpg
/path/to/your/images/image_2.jpg
/path/to/your/images/image_3.jpg
```
and use it the below scripts.
```bash
cd 01_feature_extraction
# calculate image features
./image_feature_extraction.sh
# convert txt files into binary
./features_txt2bin.sh
```
3. News article - images retrieval: 
```bash
cd 02_Similarities
./retrieval.sh
```
4. Reranking with Qwen2.5:
```bash
cd 03_Reranking
python rate_Q2.5_mediaEval2025.py
python MediaEval2025_LLM_reranking.py
```


## Citation

If you find our method useful in your work, please cite the following publication where this approach was proposed:

D. Galanopoulos, A. Goulas, V. Mezaris, "Cross-modal Image Recommendation for News Articles by Multimodal Foundation Models-based Retrieval-Reranking", 2025 Multimedia Evaluation Workshop (MediaEval'25), Dublin, Ireland.
