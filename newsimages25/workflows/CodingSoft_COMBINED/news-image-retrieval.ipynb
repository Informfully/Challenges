{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5155407,"sourceType":"datasetVersion","datasetId":2995525},{"sourceId":12714602,"sourceType":"datasetVersion","datasetId":8036075},{"sourceId":509844,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":404262,"modelId":422175}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q torch torchvision transformers\n!pip install -q pandas numpy scikit-learn\n!pip install -q pillow requests tqdm\n!pip install -q faiss-cpu # or faiss-gpu if you have GPU\n!pip install -q sentence-transformers\n\n!pip install -q git+https://github.com/openai/CLIP.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T08:59:21.421361Z","iopub.execute_input":"2025-08-12T08:59:21.421529Z","iopub.status.idle":"2025-08-12T09:01:18.573747Z","shell.execute_reply.started":"2025-08-12T08:59:21.421513Z","shell.execute_reply":"2025-08-12T09:01:18.572991Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport clip\nfrom transformers import CLIPProcessor, CLIPModel\nimport requests\nfrom tqdm import tqdm\nimport json\nimport pickle\nimport faiss\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport warnings\nwarnings.filterwarnings('ignore')\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-12T09:01:18.575431Z","iopub.execute_input":"2025-08-12T09:01:18.575675Z","iopub.status.idle":"2025-08-12T09:01:52.291354Z","shell.execute_reply.started":"2025-08-12T09:01:18.575653Z","shell.execute_reply":"2025-08-12T09:01:52.290668Z"}},"outputs":[{"name":"stderr","text":"2025-08-12 09:01:33.888306: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754989294.219333      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754989294.314817      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class Config:\n    # Paths\n    DATA_PATH = \"/kaggle/working/newsimages_data/\"\n    YFCC_EMBEDDINGS_PATH = \"/kaggle/working/yfcc_embeddings/\"\n    MODELS_PATH = \"/kaggle/working/models/\"\n    RESULTS_PATH = \"/kaggle/working/results/\"\n    \n    # Model settings\n    CLIP_MODEL_NAME = \"openai/clip-vit-base-patch32\"  # Hugging Face model\n    BATCH_SIZE = 32\n    MAX_TEXT_LENGTH = 77\n    IMAGE_SIZE = 224\n    EMBEDDING_DIM = 512\n    \n    # Inference settings\n    TOP_K_CANDIDATES = 8500\n    TARGET_IMG_SIZE = (460, 260)  # Required output size\n\n# Create directories\nfor path in [Config.DATA_PATH, Config.YFCC_EMBEDDINGS_PATH, \n             Config.MODELS_PATH, Config.RESULTS_PATH]:\n    os.makedirs(path, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T09:01:52.292118Z","iopub.execute_input":"2025-08-12T09:01:52.292694Z","iopub.status.idle":"2025-08-12T09:01:52.297876Z","shell.execute_reply.started":"2025-08-12T09:01:52.292664Z","shell.execute_reply":"2025-08-12T09:01:52.297199Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class NewsDataset:\n    def __init__(self, csv_path, images_folder_path):\n        \"\"\"Load and preprocess news articles dataset with local images\n        \n        Args:\n        csv_path: Path to the CSV file with news data\n        images_folder_path: Path to folder containing images named by image_id\n        \"\"\"\n        self.df = pd.read_csv(csv_path)\n        self.images_folder_path = images_folder_path\n        \n        print(f\"Loaded {len(self.df)} news articles\")\n        print(f\"Images folder: {images_folder_path}\")\n        print(f\"Columns: {list(self.df.columns)}\")\n        \n        # Verify expected columns are present\n        expected_cols = ['article_id', 'article_url', 'article_title', 'article_tags', 'image_id', 'image_url']\n        missing_cols = [col for col in expected_cols if col not in self.df.columns]\n        if missing_cols:\n            print(f\"Warning: Missing columns: {missing_cols}\")\n        \n        # Clean and preprocess text\n        self.df = self.preprocess_data()\n        \n        # Check available images\n        self.check_available_images()\n    \n    def check_available_images(self):\n        \"\"\"Check which images are available locally and report statistics\"\"\"\n        if not os.path.exists(self.images_folder_path):\n            print(f\"Warning: Images folder {self.images_folder_path} does not exist!\")\n            self.available_images = set()\n            return\n        \n        # Get list of available image files\n        image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}\n        available_files = []\n        \n        for file in os.listdir(self.images_folder_path):\n            if any(file.lower().endswith(ext) for ext in image_extensions):\n                # Extract image_id from filename (remove extension)\n                image_id = os.path.splitext(file)[0]\n                available_files.append(image_id)\n        \n        self.available_images = set(available_files)\n        \n        # Check how many dataset images are available\n        dataset_image_ids = set(self.df['image_id'].astype(str))\n        available_dataset_images = dataset_image_ids.intersection(self.available_images)\n        \n        print(f\"Found {len(self.available_images)} image files in folder\")\n        print(f\"Dataset has {len(dataset_image_ids)} unique image IDs\")\n        print(f\"Available images for dataset: {len(available_dataset_images)}\")\n        print(f\"Missing images: {len(dataset_image_ids) - len(available_dataset_images)}\")\n        \n        if len(available_dataset_images) < len(dataset_image_ids) * 0.1:\n            print(\"Warning: Very few images available! Check image_id naming convention.\")\n    \n    def preprocess_data(self):\n        \"\"\"Clean and preprocess the news data\"\"\"\n        df = self.df.copy()\n        \n        # Handle missing values\n        df['article_title'] = df['article_title'].fillna('')\n        df['article_tags'] = df['article_tags'].fillna('')\n        df['article_url'] = df['article_url'].fillna('')\n        df['image_url'] = df['image_url'].fillna('')\n        \n        # Combine title and tags for better matching\n        df['combined_text'] = df.apply(self.combine_text_features, axis=1)\n        \n        # Clean text\n        df['combined_text'] = df['combined_text'].str.replace('\\n', ' ')\n        df['combined_text'] = df['combined_text'].str.replace('\\r', ' ')\n        df['combined_text'] = df['combined_text'].str.strip()\n        \n        return df\n    \n    def combine_text_features(self, row):\n        \"\"\"Combine only title and tags (no web content)\"\"\"\n        parts = []\n        \n        if pd.notna(row['article_title']) and row['article_title'].strip():\n            parts.append(row['article_title'])\n        \n        if pd.notna(row['article_tags']) and row['article_tags'].strip():\n            # Clean tags and add as keywords\n            tags = row['article_tags'].replace(',', ' ').replace(';', ' ')\n            parts.append(f\"Keywords: {tags}\")\n        \n        return \". \".join(parts)\n    \n    def get_image_path(self, idx):\n        \"\"\"Get local path to image file\"\"\"\n        row = self.df.iloc[idx]\n        image_id = str(row['image_id'])\n        \n        # Check if image exists locally\n        if image_id not in self.available_images:\n            return None\n        \n        # Try different common image extensions\n        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']\n        \n        for ext in image_extensions:\n            image_path = os.path.join(self.images_folder_path, f\"{image_id}{ext}\")\n            if os.path.exists(image_path):\n                # Verify it's a valid image\n                try:\n                    with Image.open(image_path) as img:\n                        img.verify()\n                    return image_path\n                except Exception as e:\n                    print(f\"Invalid image file {image_path}: {e}\")\n                    continue\n        \n        return None\n    \n    def get_available_image_indices(self):\n        \"\"\"Get indices of rows that have available images\"\"\"\n        available_indices = []\n        \n        for idx in range(len(self.df)):\n            if self.get_image_path(idx) is not None:\n                available_indices.append(idx)\n        \n        return available_indices\n    \n    def create_filtered_dataset(self):\n        \"\"\"Create a filtered dataset containing only rows with available images\"\"\"\n        available_indices = self.get_available_image_indices()\n        \n        if not available_indices:\n            print(\"Warning: No images available! Cannot create filtered dataset.\")\n            return None\n        \n        filtered_df = self.df.iloc[available_indices].copy().reset_index(drop=True)\n        \n        # Create new dataset object with filtered data\n        filtered_dataset = NewsDataset.__new__(NewsDataset)\n        filtered_dataset.df = filtered_df\n        filtered_dataset.images_folder_path = self.images_folder_path\n        filtered_dataset.available_images = self.available_images\n        \n        print(f\"Created filtered dataset with {len(filtered_df)} articles (all have images)\")\n        \n        return filtered_dataset\n    \n    def get_article_text(self, idx):\n        \"\"\"Get preprocessed text for an article\"\"\"\n        return self.df.iloc[idx]['combined_text']\n    \n    def get_article_id(self, idx):\n        \"\"\"Get article ID\"\"\"\n        return self.df.iloc[idx]['article_id']\n    \n    def __len__(self):\n        return len(self.df)\n\nclass NewsImageRetrieval(nn.Module):\n    def __init__(self, clip_model_name=Config.CLIP_MODEL_NAME):\n        super().__init__()\n        \n        # Load CLIP model and processor\n        self.clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)  # Move to device here\n        self.processor = CLIPProcessor.from_pretrained(clip_model_name)\n        \n        # Additional layers for news domain adaptation\n        self.text_projection = nn.Linear(512, Config.EMBEDDING_DIM)\n        self.image_projection = nn.Linear(512, Config.EMBEDDING_DIM)\n        \n        # News-specific concept classifier (optional enhancement)\n        self.concept_classifier = nn.Linear(Config.EMBEDDING_DIM, 100)  # 100 news concepts\n        \n        self.dropout = nn.Dropout(0.1)\n        self.temperature = 0.07  # Use fixed temperature\n    \n    def encode_text(self, texts):\n        \"\"\"Encode text using CLIP text encoder\"\"\"\n        inputs = self.processor(text=texts, return_tensors=\"pt\", \n                                padding=True, truncation=True, max_length=Config.MAX_TEXT_LENGTH)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            text_features = self.clip_model.get_text_features(**inputs)\n        \n        # Apply additional projection for news domain\n        text_features = self.text_projection(text_features)\n        text_features = F.normalize(text_features, p=2, dim=1)\n        \n        return text_features\n    \n    def encode_image(self, images):\n        \"\"\"Encode images using CLIP image encoder\"\"\"\n        # If tensor is already normalized, bring it back to [0,1]\n        if isinstance(images, torch.Tensor):\n            # Check if values are out of [0,1]\n            if images.min() < 0 or images.max() > 1:\n                images = (images - images.min()) / (images.max() - images.min())\n            \n            # Convert to list of PIL images\n            from torchvision import transforms\n            to_pil = transforms.ToPILImage()\n            images = [to_pil(img.cpu()) for img in images]\n        \n        inputs = self.processor(images=images, return_tensors=\"pt\")\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            image_features = self.clip_model.get_image_features(**inputs)\n        \n        # Apply additional projection\n        image_features = self.image_projection(image_features)\n        image_features = F.normalize(image_features, p=2, dim=1)\n        \n        return image_features\n    \n    def forward(self, texts, images):\n        \"\"\"Forward pass for training\"\"\"\n        text_features = self.encode_text(texts)\n        image_features = self.encode_image(images)\n        \n        return text_features, image_features\n    \n    def compute_similarity(self, text_features, image_features):\n        \"\"\"Compute similarity scores between text and image features\"\"\"\n        # Normalize features\n        text_features = F.normalize(text_features, p=2, dim=1)\n        image_features = F.normalize(image_features, p=2, dim=1)\n        \n        # Compute cosine similarity\n        similarity = torch.matmul(text_features, image_features.T) / self.temperature\n        return similarity\n\nclass YFCC100MHandler:\n    \"\"\"Handle YFCC100M dataset for image retrieval with proper directory structure\"\"\"\n    \n    def __init__(self, yfcc_path):\n        self.yfcc_path = yfcc_path\n        self.image_embeddings = {}\n        self.image_metadata = {}\n        self.image_paths = {}  # Store mapping of image_id to full path\n    \n    def scan_yfcc_directory(self, max_images=None):\n        \"\"\"Scan YFCC100M directory structure and collect image paths\"\"\"\n        print(\"Scanning YFCC100M directory structure...\")\n        \n        image_paths = {}\n        total_found = 0\n        \n        # Walk through the hierarchical directory structure\n        for root, dirs, files in os.walk(self.yfcc_path):\n            for file in files:\n                if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n                    full_path = os.path.join(root, file)\n                    \n                    # Extract image ID from filename (remove extension)\n                    image_id = os.path.splitext(file)[0]\n                    image_paths[image_id] = full_path\n                    \n                    total_found += 1\n                    if max_images and total_found >= max_images:\n                        break\n            \n            if max_images and total_found >= max_images:\n                break\n        \n        self.image_paths = image_paths\n        print(f\"Found {len(image_paths)} images in YFCC100M dataset\")\n        \n        # Extract location information from directory structure (as fallback since no metadata)\n        self.extract_location_metadata()\n        \n        return image_paths\n    \n    def extract_location_metadata(self):\n        \"\"\"Extract location/landmark information from directory paths\"\"\"\n        print(\"Extracting location metadata from directory structure...\")\n        \n        metadata = {}\n        for image_id, full_path in self.image_paths.items():\n            # Get relative path from yfcc_path\n            rel_path = os.path.relpath(full_path, self.yfcc_path)\n            path_parts = rel_path.split(os.sep)\n            \n            # Extract location/landmark information\n            location_tags = []\n            for part in path_parts[:-1]:  # Exclude filename\n                if part not in ['test', 'train', 'calibration', 'images']:\n                    # Clean up landmark names (replace underscores with spaces)\n                    landmark = part.replace('_', ' ').title()\n                    location_tags.append(landmark)\n            \n            metadata[image_id] = {\n                'path': full_path,\n                'relative_path': rel_path,\n                'location_tags': location_tags,\n                'landmark': ' '.join(location_tags) if location_tags else 'Unknown'\n            }\n        \n        self.image_metadata = metadata\n        print(f\"Extracted metadata for {len(metadata)} images\")\n    \n    def generate_image_embeddings(self, model, max_images=300000, batch_size=32):\n        \"\"\"Generate embeddings for YFCC100M images using actual image files\"\"\"\n        print(f\"Generating embeddings for up to {max_images} YFCC100M images...\")\n        \n        if not self.image_paths:\n            print(\"No image paths found. Running directory scan first...\")\n            self.scan_yfcc_directory(max_images=max_images)\n        \n        embeddings = {}\n        processed = 0\n        \n        # Process images in batches\n        image_items = list(self.image_paths.items())[:max_images]\n        \n        for i in tqdm(range(0, len(image_items), batch_size), desc=\"Generating embeddings\"):\n            batch_items = image_items[i:i+batch_size]\n            batch_images = []\n            batch_ids = []\n            \n            # Load batch of images\n            for image_id, image_path in batch_items:\n                try:\n                    image = Image.open(image_path).convert('RGB')\n                    batch_images.append(image)\n                    batch_ids.append(image_id)\n                except Exception as e:\n                    print(f\"Error loading image {image_path}: {e}\")\n                    continue\n            \n            # Generate embeddings for batch\n            if batch_images:\n                try:\n                    with torch.no_grad():\n                        batch_embeddings = model.encode_image(batch_images)\n                    \n                    # Store embeddings\n                    for j, image_id in enumerate(batch_ids):\n                        embedding = batch_embeddings[j].cpu().numpy()\n                        embeddings[image_id] = embedding\n                        processed += 1\n                \n                except Exception as e:\n                    print(f\"Error generating embeddings for batch: {e}\")\n        \n        self.image_embeddings = embeddings\n        print(f\"Generated embeddings for {processed} images\")\n        return embeddings\n    \n    def get_image_info(self, image_id):\n        \"\"\"Get comprehensive information about an image\"\"\"\n        if image_id in self.image_metadata:\n            return self.image_metadata[image_id]\n        return None\n    \n    def save_embeddings(self, filepath):\n        \"\"\"Save embeddings and metadata to disk\"\"\"\n        data_to_save = {\n            'embeddings': self.image_embeddings,\n            'metadata': self.image_metadata,\n            'image_paths': self.image_paths\n        }\n        \n        with open(filepath, 'wb') as f:\n            pickle.dump(data_to_save, f)\n        print(f\"Saved {len(self.image_embeddings)} embeddings and metadata to {filepath}\")\n    \n    def load_embeddings(self, filepath):\n        \"\"\"Load embeddings and metadata from disk\"\"\"\n        with open(filepath, 'rb') as f:\n            data = pickle.load(f)\n        \n        self.image_embeddings = data.get('embeddings', {})\n        self.image_metadata = data.get('metadata', {})\n        self.image_paths = data.get('image_paths', {})\n        \n        print(f\"Loaded {len(self.image_embeddings)} embeddings and metadata from {filepath}\")\n    \n    def copy_image_for_submission(self, image_id, output_path):\n        \"\"\"Copy and resize YFCC image for submission\"\"\"\n        if image_id not in self.image_paths:\n            print(f\"Image {image_id} not found in YFCC dataset\")\n            return False\n        \n        source_path = self.image_paths[image_id]\n        \n        try:\n            with Image.open(source_path) as img:\n                # Convert to RGB if necessary\n                if img.mode != 'RGB':\n                    img = img.convert('RGB')\n                \n                # Resize to submission requirements (460x260)\n                img_resized = img.resize(Config.TARGET_IMG_SIZE, Image.Resampling.LANCZOS)\n                \n                # Save as PNG\n                img_resized.save(output_path, 'PNG')\n            return True\n        \n        except Exception as e:\n            print(f\"Error processing image {source_path}: {e}\")\n            return False\n\nclass EfficientImageRetrieval:\n    \"\"\"Efficient image retrieval using FAISS for similarity search\"\"\"\n    \n    def __init__(self, embedding_dim=Config.EMBEDDING_DIM):\n        self.embedding_dim = embedding_dim\n        self.index = None\n        self.image_ids = []\n    \n    def build_index(self, image_embeddings):\n        \"\"\"Build FAISS index for fast similarity search\"\"\"\n        print(\"Building FAISS index...\")\n        \n        # Convert embeddings to numpy array\n        embeddings_array = []\n        image_ids = []\n        \n        for img_id, embedding in image_embeddings.items():\n            embeddings_array.append(embedding)\n            image_ids.append(img_id)\n        \n        embeddings_array = np.array(embeddings_array).astype('float32')\n        \n        # Create FAISS index\n        self.index = faiss.IndexFlatIP(self.embedding_dim)  # Inner product (cosine similarity)\n        \n        # Normalize embeddings for cosine similarity\n        faiss.normalize_L2(embeddings_array)\n        \n        # Add embeddings to index\n        self.index.add(embeddings_array)\n        self.image_ids = image_ids\n        \n        print(f\"Built index with {len(image_ids)} images\")\n    \n    def search(self, query_embedding, k=Config.TOP_K_CANDIDATES):\n        \"\"\"Search for most similar images\"\"\"\n        if self.index is None:\n            raise ValueError(\"Index not built. Call build_index first.\")\n        \n        # Normalize query embedding\n        query_embedding = query_embedding.astype('float32')\n        if len(query_embedding.shape) == 1:\n            query_embedding = query_embedding.reshape(1, -1)\n        \n        faiss.normalize_L2(query_embedding)\n        \n        # Search\n        scores, indices = self.index.search(query_embedding, k)\n        \n        # Return results\n        results = []\n        for score, idx in zip(scores[0], indices[0]):\n            if idx != -1:  # Valid result\n                results.append({\n                    'image_id': self.image_ids[idx],\n                    'similarity_score': float(score)\n                })\n        \n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T09:01:52.298866Z","iopub.execute_input":"2025-08-12T09:01:52.299355Z","iopub.status.idle":"2025-08-12T09:01:52.340743Z","shell.execute_reply.started":"2025-08-12T09:01:52.299333Z","shell.execute_reply":"2025-08-12T09:01:52.339938Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class NewsImageRetrievalPipeline:\n    \"\"\"Complete pipeline for news image retrieval\"\"\"\n    \n    def __init__(self):\n        self.model = None\n        self.news_dataset = None\n        self.yfcc_handler = None\n        self.retrieval_engine = None\n    \n    def load_model(self, model_path=None):\n        \"\"\"Load trained model\"\"\"\n        self.model = NewsImageRetrieval()\n        \n        if model_path and os.path.exists(model_path):\n            self.model.load_state_dict(torch.load(model_path, map_location=device))\n            print(f\"Loaded model from {model_path}\")\n        else:\n            print(\"Using pre-trained CLIP model without fine-tuning\")\n        \n        self.model.eval()\n        self.model = self.model.to(device)\n    \n    def setup_datasets(self, news_csv_path, images_path, yfcc_path):\n        \"\"\"Setup news and YFCC datasets\"\"\"\n        self.news_dataset = NewsDataset(news_csv_path, images_path)\n        self.yfcc_handler = YFCC100MHandler(yfcc_path)\n    \n    def prepare_retrieval_index(self, embeddings_path=None):\n        \"\"\"Prepare FAISS index for efficient retrieval\"\"\"\n        if embeddings_path and os.path.exists(embeddings_path):\n            self.yfcc_handler.load_embeddings(embeddings_path)\n        else:\n            # Generate embeddings for YFCC100M dataset\n            self.yfcc_handler.generate_image_embeddings(self.model)\n            if embeddings_path:\n                self.yfcc_handler.save_embeddings(embeddings_path)\n        \n        # Build retrieval index\n        self.retrieval_engine = EfficientImageRetrieval()\n        self.retrieval_engine.build_index(self.yfcc_handler.image_embeddings)\n    \n    def retrieve_image_for_article(self, article_text, top_k=1):\n        \"\"\"Retrieve best matching image for an article\"\"\"\n        # Encode article text\n        with torch.no_grad():\n            text_embedding = self.model.encode_text([article_text])\n        \n        # Convert to numpy\n        text_embedding_np = text_embedding.cpu().numpy()[0]\n        \n        # Search for similar images\n        results = self.retrieval_engine.search(text_embedding_np, k=top_k)\n        \n        return results[0] if results else None\n    \n    def process_evaluation_set(self, article_ids, output_dir, subtask=\"LARGE\", group_name=\"CodingSoft\"):\n        \"\"\"Process articles for evaluation submission\"\"\"\n        os.makedirs(output_dir, exist_ok=True)\n        \n        results = {}\n        successful_retrievals = 0\n        \n        for article_id in tqdm(article_ids, desc=f\"Processing {subtask} articles\"):\n            # Find article in dataset\n            article_row = self.news_dataset.df[\n                self.news_dataset.df['article_id'] == article_id\n            ]\n            \n            if not article_row.empty:\n                # Get basic article text (title + tags only)\n                idx = article_row.index[0]\n                article_text = self.news_dataset.get_article_text(idx)\n                \n                # Retrieve best matching image\n                best_match = self.retrieve_image_for_article(article_text)\n                \n                if best_match:\n                    # Copy and resize image for submission\n                    output_filename = f\"{article_id}_{group_name}_CLIP.png\"\n                    output_path = os.path.join(output_dir, output_filename)\n                    \n                    success = self.yfcc_handler.copy_image_for_submission(\n                        best_match['image_id'], output_path\n                    )\n                    \n                    if success:\n                        results[article_id] = {\n                            'image_id': best_match['image_id'],\n                            'similarity_score': best_match['similarity_score'],\n                            'output_path': output_path\n                        }\n                        successful_retrievals += 1\n                    else:\n                        print(f\"Failed to copy image for article {article_id}\")\n                else:\n                    print(f\"No matching image found for article {article_id}\")\n            else:\n                print(f\"Article {article_id} not found in dataset\")\n        \n        print(f\"Successfully processed {successful_retrievals}/{len(article_ids)} articles for {subtask}\")\n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T09:01:52.341590Z","iopub.execute_input":"2025-08-12T09:01:52.341945Z","iopub.status.idle":"2025-08-12T09:01:52.364204Z","shell.execute_reply.started":"2025-08-12T09:01:52.341926Z","shell.execute_reply":"2025-08-12T09:01:52.363680Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def create_submission_structure(group_name=\"CodingSoft\"):\n    \"\"\"Create proper submission directory structure\"\"\"\n    \n    base_dir = f\"{group_name}\"\n    subdirs = [\n        \"RET_CLIP_LARGE\",\n        \"RET_CLIP_SMALL\",\n        \"RET_ENHANCED_LARGE\", \n        \"RET_ENHANCED_SMALL\"\n    ]\n    \n    for subdir in subdirs:\n        full_path = os.path.join(base_dir, subdir)\n        os.makedirs(full_path, exist_ok=True)\n        print(f\"Created directory: {full_path}\")\n\ndef resize_and_save_image(image_path, output_path, target_size=Config.TARGET_IMG_SIZE):\n    \"\"\"Resize image to submission requirements and save as PNG\"\"\"\n    try:\n        with Image.open(image_path) as img:\n            # Convert to RGB if necessary\n            if img.mode != 'RGB':\n                img = img.convert('RGB')\n            \n            # Resize to target dimensions\n            img_resized = img.resize(target_size, Image.Resampling.LANCZOS)\n            \n            # Save as PNG\n            img_resized.save(output_path, 'PNG')\n        return True\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {e}\")\n        return False\n\ndef generate_submission_files(results, group_name, approach_name, subtask):\n    \"\"\"Generate properly named submission files\"\"\"\n    group_name = \"CodingSoft\"\n    output_dir = os.path.join(group_name, f\"RET_{approach_name}_{subtask}\")\n    \n    for article_id, result in results.items():\n        # Create output filename\n        output_filename = f\"{article_id}_{group_name}_{approach_name}.png\"\n        output_path = os.path.join(output_dir, output_filename)\n        \n        # In practice, you would retrieve the actual image from YFCC100M\n        # and resize it to the required dimensions\n        # For demo purposes, we'll create a placeholder\n        \n        print(f\"Would save: {output_path}\")\n        # resize_and_save_image(source_image_path, output_path)\n\ndef setup_yfcc_dataset(yfcc_path, max_images=400000):\n    \"\"\"Setup YFCC100M dataset with proper directory scanning\"\"\"\n    print(\"Setting up YFCC100M dataset...\")\n    \n    # Initialize handler\n    yfcc_handler = YFCC100MHandler(yfcc_path)\n    \n    # Scan directory structure\n    image_paths = yfcc_handler.scan_yfcc_directory(max_images=max_images)\n    \n    # Skip loading official metadata since it's not provided\n    print(\"Skipping official metadata loading as no metadata file is provided.\")\n    \n    print(f\"YFCC100M setup complete: {len(image_paths)} images ready\")\n    return yfcc_handler\n\ndef generate_yfcc_embeddings(yfcc_handler, model, embeddings_path, max_images=300000):\n    \"\"\"Generate embeddings for YFCC100M images\"\"\"\n    print(\"Generating YFCC100M embeddings...\")\n    \n    # Generate embeddings using actual images\n    embeddings = yfcc_handler.generate_image_embeddings(\n        model=model, \n        max_images=max_images,\n        batch_size=16  # Adjust based on GPU memory\n    )\n    \n    # Save embeddings\n    yfcc_handler.save_embeddings(embeddings_path)\n    \n    return embeddings\n\ndef main_inference():\n    \"\"\"Main inference script for generating submissions\"\"\"\n    print(\"Starting NewsImages Retrieval Inference...\")\n    \n    # Initialize pipeline\n    pipeline = NewsImageRetrievalPipeline()\n    \n    # Load model (assuming it's already saved)\n    model_path = os.path.join(\"/kaggle/input/news_retrieval_model_1/pytorch/default/1\", \"news_retrieval_model.pth\")\n    pipeline.load_model(model_path)\n    \n    # Setup datasets\n    pipeline.setup_datasets(\n        news_csv_path=\"/kaggle/input/newsimagedataset-v2/newsarticles.csv\",\n        images_path=\"/kaggle/input/newsimagedataset-v2/newsimages\",\n        yfcc_path=\"/kaggle/input/yfcc100m-dataset/OANet/yfcc100m\"  # Replace with actual YFCC100M path\n    )\n    \n    # Prepare retrieval index\n    embeddings_path = os.path.join(Config.YFCC_EMBEDDINGS_PATH, \"yfcc_embeddings.pkl\")\n    pipeline.prepare_retrieval_index(embeddings_path)\n    \n    # Load evaluation article IDs (these will be provided by organizers)\n    # For now, using a sample\n    small_eval_ids = list(pipeline.news_dataset.df['article_id'].head(50))\n    large_eval_ids = list(pipeline.news_dataset.df['article_id'].head(8500))\n    \n    # Process small evaluation set\n    print(\"Processing SMALL evaluation set...\")\n    small_results = pipeline.process_evaluation_set(\n        article_ids=small_eval_ids,\n        output_dir=os.path.join(Config.RESULTS_PATH, \"RET_CLIP_SMALL\"),\n        subtask=\"SMALL\"\n    )\n    \n    # Process large evaluation set \n    print(\"Processing LARGE evaluation set...\")\n    large_results = pipeline.process_evaluation_set(\n        article_ids=large_eval_ids,\n        output_dir=os.path.join(Config.RESULTS_PATH, \"RET_CLIP_LARGE\"),\n        subtask=\"LARGE\"\n    )\n    \n    print(\"Inference completed!\")\n    return small_results, large_results\n\ndef full_pipeline_example():\n    \"\"\"Complete example of the full pipeline without training\"\"\"\n    print(\"NewsImages 2025 - Complete Pipeline Example (Skipping Training)\")\n    print(\"=\" * 50)\n    \n    # Step 1: Setup directories\n    create_submission_structure(\"CodingSoft\")\n    \n    # Skip Step 2: Training (assuming model is already saved)\n    print(\"\\nğŸ”¹ Skipping Step 2: Model training (using saved model)\")\n    \n    # Step 3: Setup YFCC100M dataset\n    print(\"\\nğŸ”¹ Step 3: Setting up YFCC100M dataset...\")\n    yfcc_path = \"/kaggle/input/yfcc100m-dataset/OANet/yfcc100m\"  # Update this path\n    yfcc_handler = setup_yfcc_dataset(yfcc_path, max_images=400000)\n    \n    # Step 4: Generate embeddings\n    print(\"\\nğŸ”¹ Step 4: Generating embeddings...\")\n    model = NewsImageRetrieval()\n    model = model.to(device)\n    embeddings_path = os.path.join(Config.YFCC_EMBEDDINGS_PATH, \"yfcc_embeddings.pkl\")\n    \n    if not os.path.exists(embeddings_path):\n        generate_yfcc_embeddings(yfcc_handler, model, embeddings_path, max_images=300000)\n    \n    # Step 5: Run inference\n    print(\"\\nğŸ”¹ Step 5: Running inference...\")\n    results = main_inference()\n    \n    print(\"\\nğŸ‰ Pipeline completed successfully!\")\n    return results\n\nif __name__ == \"__main__\":\n    print(\"NewsImages 2025 - Image Retrieval System\")\n    print(\"Directory structure detected:\")\n    print(\"natural_history_museum/\")\n    print(\"â”œâ”€â”€ train/\")\n    print(\"â”‚ â”œâ”€â”€ images/\")\n    print(\"â”‚ â”‚ â”œâ”€â”€ 11737074_...\")\n    print(\"â”‚ â”‚ â”œâ”€â”€ 12308432_...\")\n    print(\"â”‚ â”‚ â””â”€â”€ ...\")\n    print(\"â”‚ â””â”€â”€ calibration/\")\n    print(\"â””â”€â”€ test/\")\n    print(\"\\n\" + \"=\" * 50)\n    \n    # Update these paths according to your setup:\n    YFCC_PATH = \"/kaggle/input/yfcc100m-dataset/OANet/yfcc100m\"  # Update this\n    NEWS_CSV_PATH = \"/kaggle/input/newsimagedataset-v2/newsarticles.csv\"  # Update this\n    \n    print(f\"Expected YFCC100M path: {YFCC_PATH}\")\n    print(f\"Expected news CSV path: {NEWS_CSV_PATH}\")\n    print(\"\\nRunning the inference pipeline:\")\n    full_pipeline_example()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T09:01:52.365056Z","iopub.execute_input":"2025-08-12T09:01:52.365341Z","iopub.status.idle":"2025-08-12T09:30:23.993804Z","shell.execute_reply.started":"2025-08-12T09:01:52.365315Z","shell.execute_reply":"2025-08-12T09:30:23.993139Z"}},"outputs":[{"name":"stdout","text":"NewsImages 2025 - Image Retrieval System\nDirectory structure detected:\nnatural_history_museum/\nâ”œâ”€â”€ train/\nâ”‚ â”œâ”€â”€ images/\nâ”‚ â”‚ â”œâ”€â”€ 11737074_...\nâ”‚ â”‚ â”œâ”€â”€ 12308432_...\nâ”‚ â”‚ â””â”€â”€ ...\nâ”‚ â””â”€â”€ calibration/\nâ””â”€â”€ test/\n\n==================================================\nExpected YFCC100M path: /kaggle/input/yfcc100m-dataset/OANet/yfcc100m\nExpected news CSV path: /kaggle/input/newsimagedataset-v2/newsarticles.csv\n\nRunning the inference pipeline:\nNewsImages 2025 - Complete Pipeline Example (Skipping Training)\n==================================================\nCreated directory: CodingSoft/RET_CLIP_LARGE\nCreated directory: CodingSoft/RET_CLIP_SMALL\nCreated directory: CodingSoft/RET_ENHANCED_LARGE\nCreated directory: CodingSoft/RET_ENHANCED_SMALL\n\nğŸ”¹ Skipping Step 2: Model training (using saved model)\n\nğŸ”¹ Step 3: Setting up YFCC100M dataset...\nSetting up YFCC100M dataset...\nScanning YFCC100M directory structure...\nFound 36179 images in YFCC100M dataset\nExtracting location metadata from directory structure...\nExtracted metadata for 36179 images\nSkipping official metadata loading as no metadata file is provided.\nYFCC100M setup complete: 36179 images ready\n\nğŸ”¹ Step 4: Generating embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56bd13440de84ff88a6051c5fafd8896"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cda070967c5946f4a603d15901cc44f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"802be699d14545acb669f9ff52577cef"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0025e99ba79e4b799eb175ef978ac371"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0553b071ff964509b0ae76ed4bbee6be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9507624921c472f92dbf2a2fcae146a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d43ff5699cfa4b1fb73bc53d371426a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22c612be0f534d4f8d381434075ae1c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be74aeaeec4d47afbadff3eb9f49fda5"}},"metadata":{}},{"name":"stdout","text":"Generating YFCC100M embeddings...\nGenerating embeddings for up to 300000 YFCC100M images...\n","output_type":"stream"},{"name":"stderr","text":"Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2262/2262 [12:35<00:00,  2.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Generated embeddings for 36179 images\nSaved 36179 embeddings and metadata to /kaggle/working/yfcc_embeddings/yfcc_embeddings.pkl\n\nğŸ”¹ Step 5: Running inference...\nStarting NewsImages Retrieval Inference...\nLoaded model from /kaggle/input/news_retrieval_model_1/pytorch/default/1/news_retrieval_model.pth\nLoaded 8500 news articles\nImages folder: /kaggle/input/newsimagedataset-v2/newsimages\nColumns: ['article_id', 'article_url', 'article_title', 'article_tags', 'image_id', 'image_url']\nFound 8500 image files in folder\nDataset has 8500 unique image IDs\nAvailable images for dataset: 8500\nMissing images: 0\nLoaded 36179 embeddings and metadata from /kaggle/working/yfcc_embeddings/yfcc_embeddings.pkl\nBuilding FAISS index...\nBuilt index with 36179 images\nProcessing SMALL evaluation set...\n","output_type":"stream"},{"name":"stderr","text":"Processing SMALL articles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:03<00:00, 12.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Successfully processed 50/50 articles for SMALL\nProcessing LARGE evaluation set...\n","output_type":"stream"},{"name":"stderr","text":"Processing LARGE articles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8500/8500 [10:11<00:00, 13.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Successfully processed 8500/8500 articles for LARGE\nInference completed!\n\nğŸ‰ Pipeline completed successfully!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import shutil\n\n# source_dir is the folder you want to zip (e.g., /kaggle/working/my_images)\nsource_dir = \"/kaggle/working/results\"\n\n# base_name is the output zip path without the .zip extension\nshutil.make_archive(\"/kaggle/working/CodingSoft\", \"zip\", source_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T09:30:23.995901Z","iopub.execute_input":"2025-08-12T09:30:23.996138Z","iopub.status.idle":"2025-08-12T09:31:30.046530Z","shell.execute_reply.started":"2025-08-12T09:30:23.996119Z","shell.execute_reply":"2025-08-12T09:31:30.045897Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/CodingSoft.zip'"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}